{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10434b0c-f735-4da4-b025-fbe2159ae6be",
   "metadata": {},
   "source": [
    "- Bu projede arasınav için yapılan modeli gömülü cihazlarda çalıştırabilmek için Pruning, Fine-tuning, Post-quantization, Quantization Aware training yöntemleri kullanılmıştır ve arasınavdaki kodun üzerine eklenmiştir. Arasınavdan farklı olarak kodda düzenlemeler yapılmıştır.\n",
    "- Oluşturulan modelin gömülü cihazlar üzerindeki performansını test edebilmek için Edge Impulse Studio üzerinden \"Espressif ESP-EYE (ESP32 240MHz)\" cihazı için kontrol edilmiştir. Metrikler ilgili yöntemin uygulandığı kısımda yer almaktadır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1901c53f-bd48-4914-93db-f4b0694adf8f",
   "metadata": {},
   "source": [
    "# Önveri işleme(Preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f3f3ba-fdac-426c-a52e-00e93d5f11e3",
   "metadata": {},
   "source": [
    "- Cifar10 veri seti içeriğinde toplam **60.000** **32x32** pixel'lik resimler bulunduran bir veri setidir. Veri seti **10** tane kategoriye ayrılmaktadır: Uçak, Otomobil, Kuş, Kedi, Geyik, Köpek, Kurbağa, At, Gemi, Kamyon.\n",
    "- Aşağıda yer alan X değişkenindeki her bir satırdaki değerler, 32x32 pixel'lik resim için sırasıyla *kırmızı, yeşil ve mavi(RGB)* değerleri göstermektedir.\n",
    "\n",
    "- Modeli daha da küçültebilmek için label ve ilgili resimleri verisetinin içinden kaldıran kod eklendi ve işleme süresini kısaltmak için kullanıldı. Label filtering uygulandıktan sonraki metrikler quantization işleminde gösterilmiştir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "768cfc66-230d-41ee-a9a1-8945a418b2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veriseti preprocessing yapmadan once:\n",
      "Veriseti sekil ozeti\n",
      "Train: X=(50000, 32, 32, 3), y=(50000, 1)\n",
      "Test: X=(10000, 32, 32, 3), y=(10000, 1)\n",
      "\n",
      "X_test:  [[[[158 112  49]\n",
      "   [159 111  47]\n",
      "   [165 116  51]\n",
      "   ...\n",
      "   [137  95  36]\n",
      "   [126  91  36]\n",
      "   [116  85  33]]\n",
      "\n",
      "  [[152 112  51]\n",
      "   [151 110  40]\n",
      "   [159 114  45]\n",
      "   ...\n",
      "   [136  95  31]\n",
      "   [125  91  32]\n",
      "   [119  88  34]]\n",
      "\n",
      "  [[151 110  47]\n",
      "   [151 109  33]\n",
      "   [158 111  36]\n",
      "   ...\n",
      "   [139  98  34]\n",
      "   [130  95  34]\n",
      "   [120  89  33]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 68 124 177]\n",
      "   [ 42 100 148]\n",
      "   [ 31  88 137]\n",
      "   ...\n",
      "   [ 38  97 146]\n",
      "   [ 13  64 108]\n",
      "   [ 40  85 127]]\n",
      "\n",
      "  [[ 61 116 168]\n",
      "   [ 49 102 148]\n",
      "   [ 35  85 132]\n",
      "   ...\n",
      "   [ 26  82 130]\n",
      "   [ 29  82 126]\n",
      "   [ 20  64 107]]\n",
      "\n",
      "  [[ 54 107 160]\n",
      "   [ 56 105 149]\n",
      "   [ 45  89 132]\n",
      "   ...\n",
      "   [ 24  77 124]\n",
      "   [ 34  84 129]\n",
      "   [ 21  67 110]]]]\n",
      "y_test:  [[3]]\n",
      "\n",
      "\n",
      "Veriseti preprocessing yaptiktan sonra:\n",
      "Veriseti sekil ozeti\n",
      "Train: X=(50000, 32, 32, 3), y=(50000, 10)\n",
      "Test: X=(10000, 32, 32, 3), y=(10000, 10)\n",
      "\n",
      "X_test:  [[[[0.61960787 0.4392157  0.19215687]\n",
      "   [0.62352943 0.43529412 0.18431373]\n",
      "   [0.64705884 0.45490196 0.2       ]\n",
      "   ...\n",
      "   [0.5372549  0.37254903 0.14117648]\n",
      "   [0.49411765 0.35686275 0.14117648]\n",
      "   [0.45490196 0.33333334 0.12941177]]\n",
      "\n",
      "  [[0.59607846 0.4392157  0.2       ]\n",
      "   [0.5921569  0.43137255 0.15686275]\n",
      "   [0.62352943 0.44705883 0.1764706 ]\n",
      "   ...\n",
      "   [0.53333336 0.37254903 0.12156863]\n",
      "   [0.49019608 0.35686275 0.1254902 ]\n",
      "   [0.46666667 0.34509805 0.13333334]]\n",
      "\n",
      "  [[0.5921569  0.43137255 0.18431373]\n",
      "   [0.5921569  0.42745098 0.12941177]\n",
      "   [0.61960787 0.43529412 0.14117648]\n",
      "   ...\n",
      "   [0.54509807 0.38431373 0.13333334]\n",
      "   [0.50980395 0.37254903 0.13333334]\n",
      "   [0.47058824 0.34901962 0.12941177]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.26666668 0.4862745  0.69411767]\n",
      "   [0.16470589 0.39215687 0.5803922 ]\n",
      "   [0.12156863 0.34509805 0.5372549 ]\n",
      "   ...\n",
      "   [0.14901961 0.38039216 0.57254905]\n",
      "   [0.05098039 0.2509804  0.42352942]\n",
      "   [0.15686275 0.33333334 0.49803922]]\n",
      "\n",
      "  [[0.23921569 0.45490196 0.65882355]\n",
      "   [0.19215687 0.4        0.5803922 ]\n",
      "   [0.13725491 0.33333334 0.5176471 ]\n",
      "   ...\n",
      "   [0.10196079 0.32156864 0.50980395]\n",
      "   [0.11372549 0.32156864 0.49411765]\n",
      "   [0.07843138 0.2509804  0.41960785]]\n",
      "\n",
      "  [[0.21176471 0.41960785 0.627451  ]\n",
      "   [0.21960784 0.4117647  0.58431375]\n",
      "   [0.1764706  0.34901962 0.5176471 ]\n",
      "   ...\n",
      "   [0.09411765 0.3019608  0.4862745 ]\n",
      "   [0.13333334 0.32941177 0.5058824 ]\n",
      "   [0.08235294 0.2627451  0.43137255]]]]\n",
      "y_test:  [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def filter_labels(X, y, labels_to_remove):\n",
    "    # Create mask\n",
    "    mask = ~np.isin(y, labels_to_remove).flatten()\n",
    "    \n",
    "    # Apply mask\n",
    "    X_filtered = X[mask]\n",
    "    y_filtered = y[mask]\n",
    "    \n",
    "    return X_filtered, y_filtered\n",
    "\n",
    "def make_preprocessing(X_train, y_train, X_test, y_test):\n",
    "    # one hot encode uygula labellar uzerinde\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    \n",
    "    # Resim pixellerini 0-255 arasindan 0-1 arasina float olarak cek\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_train = X_train / 255.0\n",
    "    X_test = X_test / 255.0\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "# Usage example:\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "labels_to_remove = []\n",
    "#labels_to_remove = [2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Filter the datasets\n",
    "X_train, y_train = filter_labels(X_train, y_train, labels_to_remove)\n",
    "X_test, y_test = filter_labels(X_test, y_test, labels_to_remove)\n",
    "\n",
    "\n",
    "print(\"Veriseti preprocessing yapmadan once:\")\n",
    "\n",
    "print(\"Veriseti sekil ozeti\")\n",
    "print('Train: X=%s, y=%s' % (X_train.shape, y_train.shape))\n",
    "print('Test: X=%s, y=%s\\n' % (X_test.shape, y_test.shape))\n",
    "\n",
    "print(\"X_test: \", X_test[:1])\n",
    "print(\"y_test: \", y_test[:1])\n",
    "print(\"\\n\")\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = make_preprocessing(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"Veriseti preprocessing yaptiktan sonra:\")\n",
    "\n",
    "print(\"Veriseti sekil ozeti\")\n",
    "print('Train: X=%s, y=%s' % (X_train.shape, y_train.shape))\n",
    "print('Test: X=%s, y=%s\\n' % (X_test.shape, y_test.shape))\n",
    "\n",
    "print(\"X_test: \", X_test[:1])\n",
    "print(\"y_test: \", y_test[:1])\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea59e53-97a7-4a62-8844-3a32eea6d492",
   "metadata": {},
   "source": [
    "# Model Eğitimi ve Modeli Kaydetme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ca70c-44aa-421c-9bad-91d076b8fea9",
   "metadata": {},
   "source": [
    "## Katman Açıklamaları"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84797f25-6531-47f1-9012-b9e63d691ee3",
   "metadata": {},
   "source": [
    "- Used link for this model: https://www.kaggle.com/code/ektasharma/simple-cifar10-cnn-keras-code-with-88-accuracy\n",
    "\n",
    "\n",
    "1. Conv2D\n",
    "   - Görüntü verileri üzerinde konvolüsyon işlemi gerçekleştirir. Aktivasyon fonksiyonu (burada ReLU) ile çıktıları sıkıştırır, böylece modelin öğrenme yeteneğini artırır.\n",
    "2. BatchNormalization\n",
    "   - Ağdaki her katmandan gelen çıktıları normalleştirir, yani ortalamayı sıfıra ve standart sapmayı bir birimlik varyansa ayarlar. Bu, ağın daha hızlı öğrenmesine yardımcı olurken, overfitting'i azaltabilir.\n",
    "3. MaxPooling2D\n",
    "   - Her bir bölgenin maksimum değerini alarak bir örüntüyü küçültür ve özellikleri korur. Bu, ağın daha derin ve karmaşık özellikleri öğrenmesine yardımcı olurken, hesaplama maliyetini düşürür.\n",
    "4. Dropout\n",
    "   - Belirli bir olasılıkla (aşağıdaki modelde 0.3 veya 0.5) rastgele nöronları devre dışı bırakarak, modelin öğrenme sürecinde nöronların aşırı özelleşmesini önler. Bu, ağın daha genelleştirilmiş ve daha iyi performans gösteren bir model oluşturmasına yardımcı olur.\n",
    "5. Flatten\n",
    "    - CNN'de kullanılan konvolüsyon ve havuzlama katmanlarından gelen çıktılar, genellikle 2 Boyutlu veya 3 Boyutlu tensörlerdir(bir görüntünün yükseklik, genişlik ve kanal sayısı). Flatten katmanı, bu 2 Boyutlu veya 3 Boyutlu tensörleri tek boyutlu vektörlere dönüştürerek, bir sonraki katman olan Dense katmanına giriş olarak kullanılacak veri yapısını sağlar.\n",
    "6. Dense\n",
    "    - Bu katman, girişten gelen verilerle ağırlıklar arasında nokta çarpımı yapar, ardından bir aktivasyon fonksiyonu uygular ve bir çıkış üretir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4966e1d-b877-49c6-9090-e3b22d8e6813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 69s 86ms/step - loss: 1.7741 - accuracy: 0.3914 - val_loss: 1.6847 - val_accuracy: 0.4353\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 70s 89ms/step - loss: 1.2386 - accuracy: 0.5567 - val_loss: 1.0823 - val_accuracy: 0.6169\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 1.0267 - accuracy: 0.6388 - val_loss: 0.9314 - val_accuracy: 0.6742\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.9000 - accuracy: 0.6877 - val_loss: 0.9383 - val_accuracy: 0.6731\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 77s 98ms/step - loss: 0.8251 - accuracy: 0.7118 - val_loss: 0.7290 - val_accuracy: 0.7454\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.7695 - accuracy: 0.7331 - val_loss: 0.7063 - val_accuracy: 0.7555\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 75s 96ms/step - loss: 0.7311 - accuracy: 0.7489 - val_loss: 0.9182 - val_accuracy: 0.6890\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 75s 96ms/step - loss: 0.6900 - accuracy: 0.7613 - val_loss: 0.6134 - val_accuracy: 0.7910\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 80s 102ms/step - loss: 0.6616 - accuracy: 0.7732 - val_loss: 0.5648 - val_accuracy: 0.8046\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 77s 98ms/step - loss: 0.6291 - accuracy: 0.7835 - val_loss: 0.5614 - val_accuracy: 0.8097\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_model_optimization.python.core.keras.compat import keras\n",
    "from keras.losses import categorical_crossentropy\n",
    "\n",
    "# Used link for this model: https://www.kaggle.com/code/ektasharma/simple-cifar10-cnn-keras-code-with-88-accuracy\n",
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(32, 32, 3)),\n",
    "\n",
    "    keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "\n",
    "    keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(len(y_train[0]), activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss=categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "#model.save('models/cifar_model_midterm.keras')\n",
    "\n",
    "model.save('models/cifar_model_final.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca5fd6-a71a-409b-84d6-75cb62a0119a",
   "metadata": {},
   "source": [
    "# Model Dosyadan Yükleme ve Doğruluk Değeri Hesaplama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "69cc1a8c-55bb-41c5-a8d5-6005c4f0439e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Dosyadan Yukleniyor...\n",
      "\n",
      "Model Degerlendiriliyor...\n",
      "313/313 [==============================] - 6s 17ms/step - loss: 0.5614 - accuracy: 0.8097\n",
      "Dogruluk(Accuracy) Yuzdelik Oran: 80.970\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Dosyadan Yukleniyor...\\n\")\n",
    "#loaded_model = load_model('models/cifar_model_midterm.keras')\n",
    "loaded_model = keras.models.load_model('models/cifar_model_final.keras')\n",
    "\n",
    "print(\"Model Degerlendiriliyor...\")\n",
    "_, acc = loaded_model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Dogruluk(Accuracy) Yuzdelik Oran: %.3f' % (acc * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0a5f2a-48ad-44bf-b579-14e29bac51ce",
   "metadata": {},
   "source": [
    "# Model Parametreleri Analizi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fbf7d0c0-c2e0-4d22-a85e-b157c2133c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_14 (Ba  (None, 32, 32, 32)        128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_15 (Ba  (None, 32, 32, 32)        128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 16, 16, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization_16 (Ba  (None, 16, 16, 32)        128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_17 (Ba  (None, 16, 16, 64)        256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 8, 8, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_18 (Ba  (None, 8, 8, 128)         512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_19 (Ba  (None, 8, 8, 128)         512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (None, 4, 4, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               262272    \n",
      "                                                                 \n",
      " batch_normalization_20 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 552746 (2.11 MB)\n",
      "Trainable params: 551658 (2.10 MB)\n",
      "Non-trainable params: 1088 (4.25 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc347ad-6866-4cca-a05d-cccbe0c67d6b",
   "metadata": {},
   "source": [
    "- Model parametrelerinin toplamı kadar hafızaya ihtiyaç duyulur.\n",
    "- Model parametreleri aşağıdaki şekilde toplanırsa, aşağıdaki kodun sonucu kadar hafızaya ihtiyaç olduğu gözlenir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0e08bf1-cd37-4756-8768-8effdf7bb1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeli barındırmak için KB cinsinden hafıza ihtiyacı =  539.791015625\n"
     ]
    }
   ],
   "source": [
    "print(\"Modeli barındırmak için KB cinsinden hafıza ihtiyacı = \", (896+128+9248+128+0+0+128+18496+36928+256+0+0+\n",
    "                                                                  73856+512+147584+512+0+0+0+262272+512+0+1290) / 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702041a9-057f-425a-b35d-e227ae4735bc",
   "metadata": {},
   "source": [
    "- Ancak model parametrelerinin özet kısmındaki değeri alırsak: *Total params: 1,656,064*, **6.32MB** hafızaya ihtiyaç olduğu gözlenir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414418d4-2be2-4bed-ae15-1b3471fae53f",
   "metadata": {},
   "source": [
    "- 8MB(**4MB program hafızası** ve 4MB spiffs hafıza) hafızaya sahip ESP32S3 modeline göre değerlendirme yapılırsa, modelin ancak hafıza düzenlemesi(spiffs hafızadan, program hafızasına alan aktarma) yapıldıktan sonra sığabileceği gözlenir. Yada pruning(budama) işlemi ile model, >=2.32MB budandıktan sonra program hafızasına sığabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d15271a-7759-4578-8f1f-bcca1443b132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test uzerinden tahmin yapiliyor...\n",
      "313/313 [==============================] - 6s 18ms/step\n",
      "[3 8 8 ... 5 1 7]\n",
      "[3 8 8 ... 5 1 7]\n",
      "Precision: 0.8097\n",
      "Recall: 0.8097\n",
      "F1 Score: 0.8097\n",
      "Confusion Matrix:\n",
      "[[811  12  19  11  18   1   4   4 106  14]\n",
      " [  9 906   0   1   2   0   3   0  43  36]\n",
      " [ 89   1 649  35 125  25  38  14  18   6]\n",
      " [ 21   2  49 662  67  98  33  26  32  10]\n",
      " [ 11   0  29  27 872  11  17  26   7   0]\n",
      " [  9   0  39 157  48 677  12  40  15   3]\n",
      " [  8   4  25  36  55   6 842   7  17   0]\n",
      " [ 15   1  29  31  43  17   2 855   4   3]\n",
      " [ 21   6   1   1   6   2   2   1 955   5]\n",
      " [ 33  49   1   5   3   0   5   4  32 868]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "print(\"X_test uzerinden tahmin yapiliyor...\")\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(y_test)\n",
    "print(y_pred)\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='micro')\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "recall = recall_score(y_test, y_pred, average='micro')\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='micro')\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a7a2cb-37d7-4a88-906f-21455efa4a98",
   "metadata": {},
   "source": [
    "# Pruning ve Fine Tuning İşlemi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43482f1-9ed9-449e-a31f-1f2b25adf688",
   "metadata": {},
   "source": [
    "- Used link for this method: https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ad76c8d8-a4c7-4db4-9b13-6fed0f98b3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 62s 163ms/step - loss: 0.7515 - accuracy: 0.7446 - val_loss: 0.7043 - val_accuracy: 0.7638\n",
      "Model Degerlendiriliyor...\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.7588 - accuracy: 0.7435\n",
      "Dogruluk(Accuracy) Yuzdelik Oran: 74.350\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_conv2d  (None, 32, 32, 32)        1762      \n",
      " _12 (PruneLowMagnitude)                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_  (None, 32, 32, 32)        129       \n",
      " normalization_14 (PruneLow                                      \n",
      " Magnitude)                                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d  (None, 32, 32, 32)        18466     \n",
      " _13 (PruneLowMagnitude)                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_  (None, 32, 32, 32)        129       \n",
      " normalization_15 (PruneLow                                      \n",
      " Magnitude)                                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_max_po  (None, 16, 16, 32)        1         \n",
      " oling2d_6 (PruneLowMagnitu                                      \n",
      " de)                                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_dropou  (None, 16, 16, 32)        1         \n",
      " t_8 (PruneLowMagnitude)                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_  (None, 16, 16, 32)        129       \n",
      " normalization_16 (PruneLow                                      \n",
      " Magnitude)                                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d  (None, 16, 16, 64)        36930     \n",
      " _14 (PruneLowMagnitude)                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d  (None, 16, 16, 64)        73794     \n",
      " _15 (PruneLowMagnitude)                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_  (None, 16, 16, 64)        257       \n",
      " normalization_17 (PruneLow                                      \n",
      " Magnitude)                                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_max_po  (None, 8, 8, 64)          1         \n",
      " oling2d_7 (PruneLowMagnitu                                      \n",
      " de)                                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_dropou  (None, 8, 8, 64)          1         \n",
      " t_9 (PruneLowMagnitude)                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d  (None, 8, 8, 128)         147586    \n",
      " _16 (PruneLowMagnitude)                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_  (None, 8, 8, 128)         513       \n",
      " normalization_18 (PruneLow                                      \n",
      " Magnitude)                                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d  (None, 8, 8, 128)         295042    \n",
      " _17 (PruneLowMagnitude)                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_  (None, 8, 8, 128)         513       \n",
      " normalization_19 (PruneLow                                      \n",
      " Magnitude)                                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_max_po  (None, 4, 4, 128)         1         \n",
      " oling2d_8 (PruneLowMagnitu                                      \n",
      " de)                                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_dropou  (None, 4, 4, 128)         1         \n",
      " t_10 (PruneLowMagnitude)                                        \n",
      "                                                                 \n",
      " prune_low_magnitude_flatte  (None, 2048)              1         \n",
      " n_2 (PruneLowMagnitude)                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_  (None, 128)               524418    \n",
      " 4 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_  (None, 128)               513       \n",
      " normalization_20 (PruneLow                                      \n",
      " Magnitude)                                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_dropou  (None, 128)               1         \n",
      " t_11 (PruneLowMagnitude)                                        \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_  (None, 10)                2572      \n",
      " 5 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1102761 (4.21 MB)\n",
      "Trainable params: 551658 (2.10 MB)\n",
      "Non-trainable params: 551103 (2.10 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# Eger kod dogru calismaz ise preprocessing islemini yeniden yapin\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 1\n",
    "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
    "\n",
    "num_images = X_train.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                               final_sparsity=0.80,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "\n",
    "loaded_model = keras.models.load_model('models/cifar_model_final.keras')\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(loaded_model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "              loss=keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "import tempfile\n",
    "\n",
    "logdir = tempfile.mkdtemp()\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "]\n",
    "\n",
    "model_for_pruning.fit(X_train, y_train,\n",
    "                  batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "model_for_pruning.save('models/cifar_pruned_finetuned.keras')\n",
    "\n",
    "print(\"Model Degerlendiriliyor...\")\n",
    "_, acc_pruned = model_for_pruning.evaluate(X_test, y_test, verbose=1)\n",
    "print('Dogruluk(Accuracy) Yuzdelik Oran: %.3f' % (acc_pruned * 100.0))\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d0c216-6a69-4a2d-93ae-b97c47fda80b",
   "metadata": {},
   "source": [
    "# Modeli Dahada Küçültmek için Quantization İşlemi ve Tflite Dosyasına Dönüştürme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee089866-83db-43ee-8812-c0dba8516d5f",
   "metadata": {},
   "source": [
    "- Ağırlıklar ve Aktivasyon fonksiyonları int8'e quantized edildiği zaman doğruluk oranında yüksek bir düşüş(10% gibi) yaşandığı için aktivasyon fonksiyonları float32, ağırlıklar int8 olarak tutuldu.\n",
    "- Edge Impulse Studio üzerinden Espressif ESP-EYE (ESP32 240MHz) cihazı için kontrol edildiği zaman tflite modelin aşağıdaki performansa sahip olduğu görülebilir:\n",
    "PROCESSING TIME: 1323212 ms(1323.212 second).\n",
    "RAM USAGE: 83.7K\n",
    "FLASH USAGE: 592.2K\n",
    "- Ram ve flash hafıza değerleri uygun olsada işleme süresi çok uzun olmaktadır.\n",
    "\n",
    "- Label filtering(Sadece Uçak ve Otomobil) uyguladıktan sonra modelin performansı aşağıdaki gibi olmaktadır:\n",
    "PROCESSING TIME: 1268225 ms.\n",
    "RAM USAGE: 83.7K\n",
    "FLASH USAGE: 591.2K\n",
    "- Yani label filtering işlemi, işleme süresinin azalmasına yardımcı olmadı. 2 tane label bırakıldıktan sonra doğruluk oranı %96 civarına çıktı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "99ddd1ac-6170-47a2-8524-4fbb57a108db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmplgiw50cy/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmplgiw50cy/assets\n",
      "/home/d3v3lop3r/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:964: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1717927310.686009   13266 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1717927310.686019   13266 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2024-06-09 13:01:50.686128: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmplgiw50cy\n",
      "2024-06-09 13:01:50.687971: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-06-09 13:01:50.687984: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmplgiw50cy\n",
      "2024-06-09 13:01:50.704112: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
      "2024-06-09 13:01:50.752197: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: /tmp/tmplgiw50cy\n",
      "2024-06-09 13:01:50.768342: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 82217 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow_model_optimization.python.core.keras.compat import keras\n",
    "import numpy as np\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    for _ in range(100):\n",
    "        # Get a random batch of images from your dataset\n",
    "        data = X_train[np.random.choice(X_train.shape[0], 1, replace=False)]\n",
    "        yield [data.astype(np.float32)]\n",
    "\n",
    "converter = tensorflow.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "converter.optimizations = [tensorflow.lite.Optimize.DEFAULT]\n",
    "\n",
    "\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "\n",
    "\n",
    "# Use mixed precision quantization (weights in int8, activations in float32)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # allow float32 activations\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS_INT8  # allow int8 weights\n",
    "]\n",
    "\n",
    "\n",
    "# When applying below code the accuracy drop significantly(10%) - Full Integer Quantization\n",
    "'''\n",
    "# Specify the target_spec to ensure full integer quantization\n",
    "converter.target_spec.supported_ops = [tensorflow.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "# Ensure the input and output tensors are int8\n",
    "converter.inference_input_type = tensorflow.int8\n",
    "converter.inference_output_type = tensorflow.int8\n",
    "'''\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "with open('quantized_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99d4dc9-5ac3-4a03-b36e-5ec50be0d448",
   "metadata": {},
   "source": [
    "# Tflite Modelin Doğruluk Oranının Hesaplanması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "88612d70-f981-4796-8b7a-b859f88f072e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy after quantization: 71.26%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Eger label filtering yapildiysa preprocessing isleminde burada tekrar cifar10 veri setini \"Prepare the test dataset\" \n",
    "# kisminda oldugu gibi yuklemek accuracy'nin dogru bulunamamasına yol acabilir.\n",
    "\n",
    "# Load the quantized TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"quantized_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Prepare the test dataset\n",
    "(_, _), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Function to run inference on a single input\n",
    "def run_inference(input_data):\n",
    "    input_data = np.expand_dims(input_data, axis=0).astype(np.float32)\n",
    "    #input_data = np.expand_dims(input_data, axis=0).astype(np.int8)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return output_data\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = 0\n",
    "total_predictions = len(X_test)\n",
    "\n",
    "for i in range(total_predictions):\n",
    "    input_data = X_test[i]\n",
    "    true_label = np.argmax(y_test[i])\n",
    "    \n",
    "    # Get model prediction\n",
    "    output_data = run_inference(input_data)\n",
    "    predicted_label = np.argmax(output_data)\n",
    "    \n",
    "    if predicted_label == true_label:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Model accuracy after quantization: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa80e37-b807-4dbe-918f-c8ce13983359",
   "metadata": {},
   "source": [
    "# Doğruluk Oranını Arttırmak için Quantization Aware Training İşleminin Denenmesi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06681f22-60e8-406f-b82e-71e0bd74feb0",
   "metadata": {},
   "source": [
    "- Bu yöntem post-quantization yaparken full-integer quantization yaptıktan sonra doğruluk oranının ciddi bir şekilde düşmesinden ötürü denedi.\n",
    "- Quantization Aware Training yaparken BatchNormalization katmanından dolayı hata verdiğinden ötürü yeni model katmanları kullanılarak model oluşturuldu.\n",
    "- Used link for this model: https://github.com/Ermlab/cifar10keras\n",
    "- Used link for this quantization method: https://www.tensorflow.org/model_optimization/guide/quantization/training_example.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b45e667-99c1-4fba-a2ac-19aa99e9ee75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 64s 81ms/step - loss: 1.9559 - accuracy: 0.2809 - val_loss: 1.6377 - val_accuracy: 0.4383\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 1.4916 - accuracy: 0.4581 - val_loss: 1.3321 - val_accuracy: 0.5136\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 77s 98ms/step - loss: 1.2905 - accuracy: 0.5362 - val_loss: 1.1999 - val_accuracy: 0.5645\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 76s 98ms/step - loss: 1.1463 - accuracy: 0.5892 - val_loss: 1.0674 - val_accuracy: 0.6177\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 74s 95ms/step - loss: 1.0100 - accuracy: 0.6418 - val_loss: 0.9233 - val_accuracy: 0.6703\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.8990 - accuracy: 0.6821 - val_loss: 0.8311 - val_accuracy: 0.7076\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 76s 97ms/step - loss: 0.8028 - accuracy: 0.7177 - val_loss: 0.7942 - val_accuracy: 0.7232\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.7320 - accuracy: 0.7422 - val_loss: 0.8557 - val_accuracy: 0.7069\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 77s 99ms/step - loss: 0.6665 - accuracy: 0.7661 - val_loss: 0.7101 - val_accuracy: 0.7569\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.6077 - accuracy: 0.7858 - val_loss: 0.6914 - val_accuracy: 0.7606\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer_2 (Quantize  (None, 32, 32, 3)         3         \n",
      " Layer)                                                          \n",
      "                                                                 \n",
      " quant_conv2d_12 (QuantizeW  (None, 32, 32, 32)        963       \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_dropout_10 (Quantize  (None, 32, 32, 32)        1         \n",
      " WrapperV2)                                                      \n",
      "                                                                 \n",
      " quant_conv2d_13 (QuantizeW  (None, 32, 32, 32)        9315      \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_max_pooling2d_6 (Qua  (None, 16, 16, 32)        1         \n",
      " ntizeWrapperV2)                                                 \n",
      "                                                                 \n",
      " quant_conv2d_14 (QuantizeW  (None, 16, 16, 64)        18627     \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_dropout_11 (Quantize  (None, 16, 16, 64)        1         \n",
      " WrapperV2)                                                      \n",
      "                                                                 \n",
      " quant_conv2d_15 (QuantizeW  (None, 16, 16, 64)        37059     \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_max_pooling2d_7 (Qua  (None, 8, 8, 64)          1         \n",
      " ntizeWrapperV2)                                                 \n",
      "                                                                 \n",
      " quant_conv2d_16 (QuantizeW  (None, 8, 8, 128)         74115     \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_dropout_12 (Quantize  (None, 8, 8, 128)         1         \n",
      " WrapperV2)                                                      \n",
      "                                                                 \n",
      " quant_conv2d_17 (QuantizeW  (None, 8, 8, 128)         147843    \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_max_pooling2d_8 (Qua  (None, 4, 4, 128)         1         \n",
      " ntizeWrapperV2)                                                 \n",
      "                                                                 \n",
      " quant_flatten_2 (QuantizeW  (None, 2048)              1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_dropout_13 (Quantize  (None, 2048)              1         \n",
      " WrapperV2)                                                      \n",
      "                                                                 \n",
      " quant_dense_4 (QuantizeWra  (None, 1024)              2098181   \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_dropout_14 (Quantize  (None, 1024)              1         \n",
      " WrapperV2)                                                      \n",
      "                                                                 \n",
      " quant_dense_5 (QuantizeWra  (None, 10)                10255     \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2396370 (9.14 MB)\n",
      "Trainable params: 2395434 (9.14 MB)\n",
      "Non-trainable params: 936 (3.66 KB)\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 4s 1s/step - loss: 0.7571 - accuracy: 0.7156 - val_loss: 0.4268 - val_accuracy: 0.8900\n",
      "Baseline test accuracy: 0.7605999708175659\n",
      "Quant test accuracy: 0.7415000200271606\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpyo93diu3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpyo93diu3/assets\n",
      "/home/d3v3lop3r/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:964: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1717853204.584972  134275 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1717853204.584994  134275 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2024-06-08 16:26:44.585141: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpyo93diu3\n",
      "2024-06-08 16:26:44.589804: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-06-08 16:26:44.589821: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpyo93diu3\n",
      "2024-06-08 16:26:44.627249: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
      "2024-06-08 16:26:44.748744: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: /tmp/tmpyo93diu3\n",
      "2024-06-08 16:26:44.782131: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 196991 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow_model_optimization.python.core.keras.compat import keras\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# one hot encode uygula labellar uzerinde\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Resim pixellerini 0-255 arasindan 0-1 arasina float olarak cek\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Batch Normalization is must be removed for Quantization Aware \n",
    "# Training so below model is used without batch normalization\n",
    "\n",
    "# Used link for this model: https://github.com/Ermlab/cifar10keras\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(1024, activation='relu', kernel_constraint=keras.constraints.max_norm(3)),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(len(y_train[0]), activation='softmax')\n",
    "])\n",
    "\n",
    "sgd = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=False)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# q_aware stands for for quantization aware.\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "              loss=keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()\n",
    "\n",
    "train_images_subset = X_train[0:1000] # out of 60000\n",
    "train_labels_subset = y_train[0:1000]\n",
    "\n",
    "q_aware_model.fit(train_images_subset, train_labels_subset,\n",
    "                  batch_size=500, epochs=1, validation_split=0.1)\n",
    "\n",
    "_, baseline_model_accuracy = model.evaluate(\n",
    "    X_test, y_test, verbose=0)\n",
    "\n",
    "_, q_aware_model_accuracy = q_aware_model.evaluate(\n",
    "   X_test, y_test, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "print('Quant test accuracy:', q_aware_model_accuracy)\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    for _ in range(100):\n",
    "        # Get a random batch of images from your dataset\n",
    "        data = X_train[np.random.choice(X_train.shape[0], 1, replace=False)]\n",
    "        yield [data.astype(np.float32)]\n",
    "\n",
    "converter = tensorflow.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "converter.optimizations = [tensorflow.lite.Optimize.DEFAULT]\n",
    "\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "\n",
    "# Specify the target_spec to ensure full integer quantization\n",
    "converter.target_spec.supported_ops = [tensorflow.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "# Ensure the input and output tensors are int8\n",
    "converter.inference_input_type = tensorflow.int8\n",
    "converter.inference_output_type = tensorflow.int8\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "with open('quantized_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e52b78-6d68-4181-8f38-2fd23766e5cf",
   "metadata": {},
   "source": [
    "## Quantization Aware Training ile Oluşturulan Tflite Dosyasının Doğruluk Oranını Bulma İşlemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aad948b3-bcee-4c80-873a-2c2e703d93ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy after quantization: 10.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the quantized TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"quantized_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Prepare the test dataset\n",
    "(_, _), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Function to run inference on a single input\n",
    "def run_inference(input_data):\n",
    "    input_data = np.expand_dims(input_data, axis=0).astype(np.int8)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return output_data\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = 0\n",
    "total_predictions = len(X_test)\n",
    "\n",
    "for i in range(total_predictions):\n",
    "    input_data = X_test[i]\n",
    "    true_label = np.argmax(y_test[i])\n",
    "    \n",
    "    # Get model prediction\n",
    "    output_data = run_inference(input_data)\n",
    "    predicted_label = np.argmax(output_data)\n",
    "    \n",
    "    if predicted_label == true_label:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Model accuracy after quantization: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c28ce-e4f4-47ab-9d09-b7e4a07450a3",
   "metadata": {},
   "source": [
    "- Doğruluk oranı %10 olarak belirlendi. Post-quantization yöntemi ile aynı doğruluk değeri döndürdüğünden ötürü quantization aware training işlemi de full-integer quantization işlemi için doğruluk oranını arttırmadı."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd102f3-7ab8-41f8-84d9-fa26d9bb812e",
   "metadata": {},
   "source": [
    "# Modelin İşleme Hızını Azaltabilmek için Yeni Parametreler ile Modelin Eğitilmesi ve Pruning, Fine-tuning ve Post-quantization İşlemlerinin Yeniden Uygulanması "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa79dd5f-4138-40b9-bacd-001b4ffc7ffb",
   "metadata": {},
   "source": [
    "- Yeni modelin parametreleri için kullanılan link: https://studio.edgeimpulse.com/public/51070/latest/acquisition/training?page=1\n",
    "- Edge Impulse üzerinde eğitilmiş modelin performans değerli şu şekilde verilmektedir:\n",
    "ACCURACY: 74.4%\n",
    "INFERENCING TIME: 1251 ms.\n",
    "PEAK RAM USAGE: 44.7K\n",
    "FLASH USAGE: 308.2K\n",
    "- Aynı parametreler üzerinde denenerek, benzer doğruluk oranı ve işlem hızına ulaşılmaya çalışıldı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "37e7bc8e-942f-4327-a7e7-02dcc03705c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 12s 74ms/step - loss: 0.4421 - accuracy: 0.8123 - val_loss: 0.2960 - val_accuracy: 0.8720\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 13s 80ms/step - loss: 0.2615 - accuracy: 0.8896 - val_loss: 0.2428 - val_accuracy: 0.8955\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 13s 80ms/step - loss: 0.2149 - accuracy: 0.9128 - val_loss: 0.2104 - val_accuracy: 0.9075\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 13s 80ms/step - loss: 0.1871 - accuracy: 0.9266 - val_loss: 0.2042 - val_accuracy: 0.9150\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 12s 78ms/step - loss: 0.1407 - accuracy: 0.9449 - val_loss: 0.2026 - val_accuracy: 0.9210\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 12s 79ms/step - loss: 0.1088 - accuracy: 0.9613 - val_loss: 0.2375 - val_accuracy: 0.9080\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 12s 77ms/step - loss: 0.0847 - accuracy: 0.9698 - val_loss: 0.2249 - val_accuracy: 0.9170\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 12s 76ms/step - loss: 0.0603 - accuracy: 0.9809 - val_loss: 0.3056 - val_accuracy: 0.9025\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 12s 78ms/step - loss: 0.0401 - accuracy: 0.9884 - val_loss: 0.2677 - val_accuracy: 0.9200\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 13s 80ms/step - loss: 0.0230 - accuracy: 0.9947 - val_loss: 0.2824 - val_accuracy: 0.9235\n",
      "Model Dosyadan Yukleniyor...\n",
      "\n",
      "Model Degerlendiriliyor...\n",
      "63/63 [==============================] - 1s 6ms/step - loss: 0.2824 - accuracy: 0.9235\n",
      "Dogruluk(Accuracy) Yuzdelik Oran: 92.350\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_20 (Conv2D)          (None, 32, 32, 32)        320       \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 32, 32, 64)        6208      \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 65536)             0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                4194368   \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4201026 (16.03 MB)\n",
      "Trainable params: 4201026 (16.03 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "71/71 [==============================] - 9s 113ms/step - loss: 0.0184 - accuracy: 0.9954 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
      "Model Degerlendiriliyor...\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.3120 - accuracy: 0.9240\n",
      "Dogruluk(Accuracy) Yuzdelik Oran: 92.400\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_conv2d  (None, 32, 32, 32)        610       \n",
      " _20 (PruneLowMagnitude)                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d  (None, 32, 32, 64)        12354     \n",
      " _21 (PruneLowMagnitude)                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_flatte  (None, 65536)             1         \n",
      " n_4 (PruneLowMagnitude)                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_  (None, 64)                8388674   \n",
      " 8 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_  (None, 2)                 260       \n",
      " 9 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8401899 (32.05 MB)\n",
      "Trainable params: 4201026 (16.03 MB)\n",
      "Non-trainable params: 4200873 (16.03 MB)\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpbgldadg1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpbgldadg1/assets\n",
      "/home/d3v3lop3r/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:964: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1717931610.824218   13266 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1717931610.824230   13266 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2024-06-09 14:13:30.824340: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpbgldadg1\n",
      "2024-06-09 14:13:30.824766: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-06-09 14:13:30.824775: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpbgldadg1\n",
      "2024-06-09 14:13:30.828173: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
      "2024-06-09 14:13:30.849789: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: /tmp/tmpbgldadg1\n",
      "2024-06-09 14:13:30.856323: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 31981 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy after quantization: 18.47%\n"
     ]
    }
   ],
   "source": [
    "### Preprocessing islemi ###\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def filter_labels(X, y, labels_to_remove):\n",
    "    # Create mask\n",
    "    mask = ~np.isin(y, labels_to_remove).flatten()\n",
    "    \n",
    "    # Apply mask\n",
    "    X_filtered = X[mask]\n",
    "    y_filtered = y[mask]\n",
    "    \n",
    "    return X_filtered, y_filtered\n",
    "\n",
    "def make_preprocessing(X_train, y_train, X_test, y_test):\n",
    "    # one hot encode uygula labellar uzerinde\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    \n",
    "    # Resim pixellerini 0-255 arasindan 0-1 arasina float olarak cek\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_train = X_train / 255.0\n",
    "    X_test = X_test / 255.0\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "labels_to_remove = []\n",
    "labels_to_remove = [2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Filter the datasets\n",
    "X_train, y_train = filter_labels(X_train, y_train, labels_to_remove)\n",
    "X_test, y_test = filter_labels(X_test, y_test, labels_to_remove)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = make_preprocessing(X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Model egitme islemi ###\n",
    "from tensorflow_model_optimization.python.core.keras.compat import keras\n",
    "from keras.losses import categorical_crossentropy\n",
    "\n",
    "# Used link for this model: https://studio.edgeimpulse.com/public/51070/latest/acquisition/training?page=1\n",
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(32, 32, 3)),\n",
    "\n",
    "    keras.layers.Conv2D(32, (3, 1), padding='same', activation='relu'),\n",
    "    keras.layers.Conv2D(64, (3, 1), padding='same', activation='relu'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(len(y_train[0]), activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss=categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "model.save('models/cifar_model_final_2.keras')\n",
    "\n",
    "print(\"Model Dosyadan Yukleniyor...\\n\")\n",
    "loaded_model = keras.models.load_model('models/cifar_model_final_2.keras')\n",
    "\n",
    "print(\"Model Degerlendiriliyor...\")\n",
    "_, acc = loaded_model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Dogruluk(Accuracy) Yuzdelik Oran: %.3f' % (acc * 100.0))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Pruning ve Fine-tuning islemi ###\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 1\n",
    "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
    "\n",
    "num_images = X_train.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                               final_sparsity=0.80,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "\n",
    "loaded_model = keras.models.load_model('models/cifar_model_final_2.keras')\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(loaded_model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "              loss=keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "import tempfile\n",
    "\n",
    "logdir = tempfile.mkdtemp()\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "]\n",
    "\n",
    "model_for_pruning.fit(X_train, y_train,\n",
    "                  batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "model_for_pruning.save('models/cifar_pruned_finetuned_2.keras')\n",
    "\n",
    "print(\"Model Degerlendiriliyor...\")\n",
    "_, acc_pruned = model_for_pruning.evaluate(X_test, y_test, verbose=1)\n",
    "print('Dogruluk(Accuracy) Yuzdelik Oran: %.3f' % (acc_pruned * 100.0))\n",
    "\n",
    "model_for_pruning.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Quantization islemi ###\n",
    "import tensorflow\n",
    "from tensorflow_model_optimization.python.core.keras.compat import keras\n",
    "import numpy as np\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    for _ in range(100):\n",
    "        # Get a random batch of images from your dataset\n",
    "        data = X_train[np.random.choice(X_train.shape[0], 1, replace=False)]\n",
    "        yield [data.astype(np.float32)]\n",
    "\n",
    "converter = tensorflow.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "converter.optimizations = [tensorflow.lite.Optimize.DEFAULT]\n",
    "\n",
    "\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "\n",
    "\n",
    "# Use mixed precision quantization (weights in int8, activations in float32)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # allow float32 activations\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS_INT8  # allow int8 weights\n",
    "]\n",
    "\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "with open('quantized_model_2.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Tflite modelinin dogruluk degerini kontrol etme ###\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the quantized TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"quantized_model_2.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Prepare the test dataset\n",
    "(_, _), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Function to run inference on a single input\n",
    "def run_inference(input_data):\n",
    "    input_data = np.expand_dims(input_data, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return output_data\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = 0\n",
    "total_predictions = len(X_test)\n",
    "\n",
    "for i in range(total_predictions):\n",
    "    input_data = X_test[i]\n",
    "    true_label = np.argmax(y_test[i])\n",
    "    \n",
    "    # Get model prediction\n",
    "    output_data = run_inference(input_data)\n",
    "    predicted_label = np.argmax(output_data)\n",
    "    \n",
    "    if predicted_label == true_label:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Model accuracy after quantization: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1f6318-77c2-4af4-9c2a-bd99193cf97d",
   "metadata": {},
   "source": [
    "- Full-integer quantization yaptıktan sonra modelin doğruluk değeri gene %10 seviyesine düştü. Bundan dolayı yukarıdaki kodda yapıldığı şekilde sadece ağırlıkları quantize edecek şekilde yapıldı ancak böylelikle doğruluk değeri yukarıda olduğu gibi 18.47% olarak belirlendi.\n",
    "- Bu model ile Edge Impulse Studio ile eğitilen modelde olduğu gibi bir doğruluk oranına ulaşılamadı. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "title": "TinyML Final Project"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
